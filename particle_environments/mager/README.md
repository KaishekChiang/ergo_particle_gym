# MAGER
Multiple-Agent Gym Environments with Risk is a package containing scenarios used for multiple-agent reinforcement learning with additional risk-taking, and, -exploitation capabilities.

Author: Eduardo Andrade

## Summary
Includes an environment derived from the one in [multiagent-particle-envs](../multiagent-particle-envs/multiagent/environment.py) and a single scenario based on [simple_spread.py](../multiagent-particle-envs/multiagent/scenarios/simple_spread.py).

**Currently does not work--refer to [issues](#issues)**

## Purpose
The idea was to build a scenario that involved risk-taking based off the work having been done with multiple agents in open AI (https://github.com/openai/multiagent-particle-envs). 

In order to facilitate benchmarking and comparison of the scenarios with respect to any improvements generated by risk exploitation, this scenario and environment would need to be launched from an integrated script that could also initiate those scenarios and environments previously developed. See integrated [test script](../env_integration_test.py)

## Scenario
One scenario has been implemented. It adds an obstacle to the environment and flags one of the landmarks as the goal landmark--this is all unknown to all the agents. If an agent collides with the goal landmark, the agents become aware of it and they are subsequently rewarded based on their distance to it. Secondly, if an agent collides with the obstacle it will be lost, and the location of that obstacle will be relayed to the rest of the swarm. Rewards will then be allotted based on distance from the obstacle (more reward the further the agents are from the obstacle).

## Current State
`scenarios/ergo_hazards.py`:
  1. needs to have proper observations generated (which include communicated data)
  2. reward function is incomplete

## Running
In order to test the environment and scenario, run the test script as follows:

```bash
cd theory_sandbox/ergo_gym
python env_integration_test.py --environment MultiAgentRiskEnv --scenario ergo_spread --display
```

## Issues
Resolved:
``` markdown
Our conception of risk requires the sacrifice of an agent, e.g., due to colliding with an obstacle previously undetected to the entire swarm.
Removing the agent from the world is easy enough, but the issue arises in the integrated test script. 

The use of tensorflow for learning naturally separates the learning algorithm from the test script--which is desirable.
However, before each step through the environment, actions are chosen by a ["trainer"](../maddpg/maddpg/trainer/nonlearning.py).
These trainers are initiated before the loop with a predetermined number of agents and observation space shape.
Therefore, after each sacrifice of any agent, the trainers must be updated.
I attempted to update the trainers by explicitly modifying them and this proved very difficult.
Secondly, I tried to re-initialize the trainers after sacrifices and this presented issues.

"Killing" the sacrificed agent in the script proved to be a labyrinth which I was not able to navigate--I am not experienced with Tensorflow.
As it seems, sacrifice of an agent in regards to the trainer is much more complicated than I anticipated.
```

